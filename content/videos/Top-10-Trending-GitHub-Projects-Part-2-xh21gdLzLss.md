---
title: "Top 10 Trending GitHub Projects (Part 2)"
video_id: xh21gdLzLss
date: 2025-12-01
url: https://youtu.be/xh21gdLzLss
tags: [ai, github, opensource, tools]
---

Welcome back, developers. The
open-source AI revolution isn't slowing
down. We're back with the projects that
are taking over GitHub. In this video,
we cover the top 10 trending GitHub
projects this week. Part two. If you
want to run powerful AI models
efficiently and build a genenic
workflows, this list is essential. Every
project link in detail is below. Stick
around for llama.cpp at the end. It's
the foundation of local AI. Let's dive
in. Welcome back to Manu AGI tutorials.
Here we explore the exciting world of
AI, latest AI tools for you. So don't
forget to hit that subscribe button and
the notification bell so you don't miss
out on the latest AI insights. So let's
start today's video.
>> Project number one, Beta Fish
Multi-agent Public Opinion Intelligence
System. Let's dive into the powerful
GitHub project Beta Fish, an open-source
multi- aent public opinion analysis
assistant built from scratch to help
anyone break out of their information
bubble. Betaish continuously collects
data from major domestic and
international platforms, monitoring real
discussions, comments, and trends
instead of relying on limited or biased
snapshots, so you get a fuller and more
accurate picture of what people truly
think. Its core strength is a
multi-agent forum style intelligence
engine. Different specialized agents
with their own tools and thinking styles
debate, cross-check, and refine
insights, which reduces hallucinations
and avoids one-dimensional conclusions.
It goes beyond simple sentiment labels
by generating deep structured analysis
reports showing narrative directions,
sentiment shifts, crises, opportunities,
and future trend predictions that
decision makers, brands, and
institutions can use directly.
Technically, Beta Fish is lightweight,
modular, implemented in pure Python
without relying on heavy frameworks,
making it easier for developers to
deploy, customize agents, plug in
different models, and extend it into
their own monitoring or decision support
products. In short, Beta Fish is not
just a crawler or a dashboard. It is a
smart collaborative AI analysis system
that transforms raw social noise into
trustworthy multi-angle insights for
anyone who needs serious real-time
public opinion intelligence. Project
number two, Skyvern Visionowered
autonomous web automation. Let's dive
into the next powerful GitHub project,
Skyvern. An AI web agent framework
designed to fully automate browserbased
workflows using LLMs and computer vision
instead of fragile handwritten
selectors. Skyvern understands web pages
visually, plans actions like a human,
and executes end-to-end tasks such as
form filling, login flows, file
downloads, data extraction, lead
generation, QAX, and repetitive SAS
operations across many different
websites with a single natural language
instruction. Unlike traditional RPA or
Axe POBOP scripts that break whenever a
layout changes, Skyvern uses a swarm of
agents plus Playright powered browser
control to dynamically inspect the UI,
adapt to new pages, and reuse the same
workflow template on multiple domains,
making it far more robust and scalable.
It offers a clean API and UI so teams
can trigger tasks programmatically or
through a dashboard. supports both
self-hosted deployment and Skyvern Cloud
with antibbot proxy and capture handling
and delivers structured outputs suitable
for production pipelines. Backed by
strong benchmark results on Webbench and
an active AGPL licensed open- source
codebase with thousands of stars,
Skyvern stands out as a serious choice
for anyone who wants modern
vision-driven web automation that feels
less like scripting a browser and more
like managing a reliable autonomous
operator for real world workflows.
Project number three, Local AI. Free
open-source OpenAI compatible local AI
stack. Let's dive into the another
powerful GitHub project, Local AI. A
free and open- source alternative to
Cloud AI APIs that lets you run
language, image, and audio models
entirely on your own hardware while
keeping full control of your data. Local
AI acts as a drop-in replacement for the
OpenAI style API. So most existing
tools, SDKs, and libraries can work with
it by just switching the endpoint, which
makes integration extremely simple for
developers. It supports multiple model
backends and families, including popular
open-source LLMs and image models, and
can run efficiently on regular consumer
machines without requiring expensive
GPUs, making serious AI capabilities
accessible for indie developers, small
teams, and on-prem setups. Beyond being
an API wrapper, local AI is part of a
bigger local first ecosystem with local
AGI for autonomous agents and local
recall for semantic search and memory.
So you can build full agentic workflows,
knowledge bases, and tools that never
leave your environment. It is MIT
licensed, actively maintained by the
community. Offers a web UI model
explorer and simple configuration
through YAML so developers can quickly
spin up their own private AI platform
for chat generation, ROG, automation or
internal products without vendor lockin
or data leaks. Project number four,
Agent Lightning. Train any AI agent with
reinforcement learning and zero code
integration. Let's dive into the next
powerful GitHub project, Agent
Lightning. An open-source framework from
Microsoft designed to make training AI
agents practical, standardized, and
productionready without forcing you to
rewrite your whole stack. At its core,
Agent Lightning lets you plug in almost
any existing agent built with Langchain,
Autogen, OpenAI agent SDK, Crew AI,
Microsoft Agent Framework, or even plain
Python and turn it into an optimizable
training ready agent using a lightweight
tracing API. So you can start logging
trajectories, rewards, and outcomes with
almost zero code changes. It supports
multiple training strategies, including
reinforcement learning, supervised
fine-tuning, and automatic prompt
optimization, and can selectively
optimize one or more agents inside a
larger multi- aent system instead of
forcing a full rewrite. All collected
interactions flow into the Lightning
Store, a central hub that keeps tasks,
logs, and metrics aligned, making it
easier to visualize experiments, debug
behaviors, and compare policies across
runs. With active development, recent
stable releases, MIT license examples,
and integrations with real workloads
like SQL writing agents, and complex
reasoning tasks, Agent Lightning stands
out as a serious trainer layer for teams
who want to systematically improve their
agents performance in the real world
instead of relying only on static
prompts or manual tweaking. Project
number five, deep code. Open agentic
coding from ideas to productionready
code. Let's dive into the next powerful
GitHub project, Deep Code. An
open-source multi-agent coding system
from HKU that turns research papers,
text prompts, and high-level ideas into
real productionready code with minimal
manual effort. Deep code is built around
three core capabilities. Paper 2 code,
which can read complex academic papers
and implement their algorithms
correctly. Text to web, which converts
simple descriptions into working
front-end code. and text two backend
which generates robust back-end services
from plain language requirements. Under
the hood, it uses an autonomous
self-chestrating team of agents to
handle planning, coding, debugging, and
verification. So instead of copy pasting
from PDFs or struggling with scattered
tools, you get an end-to-end pipeline
from concept to clean structured
implementation. What makes Deep Code
stand out is not just features, but
proof. on OpenAI's paperbench benchmark.
It beats top machine learning PhDs and
leading commercial code agents by a huge
margin, showing that the architecture
really works for serious complex coding
tasks. It comes with both a CLI and a
modern web dashboard, supports
multimodal inputs like PDFs and URLs,
and is released under the MIT license so
teams can integrate it into real
workflows, customize agents, and build
their own AI coding lab on top of it. In
short, Deep Code is designed for
developers, researchers, and startups
who want to go beyond simple
autocomplete and use an intelligent,
verifiable agentic system that can
reliably ship full stack code from real
world specifications. Project number
six, Chef, the AI app builder that
actually understands backend. Let's dive
into the next powerful GitHub project,
Chef. an open- source AI app builder
from Convex that doesn't just generate
UI mockups, but builds full stack web
apps with a real backend that works in
production. Chef is tightly integrated
with Convex, a reactive database and
back-end platform, which means the AI
has native knowledge of database
operations, server functions, O file
uploads, and real-time updates. Instead
of guessing how your stack works, you
describe what you want. dashboards,
internal tools, SAS style workflows,
admin panels, and Chef wires up
front-end components, back-end logic,
and convex APIs together into a coherent
codebase you can inspect, edit, and
deploy. Unlike generic AI coding tools
that spit out isolated snippets, Chef
uses a consistent architecture with
strong data models and live reactivity,
so the generated app is easier to
maintain and extend. It's open- source
under Apache 2.0, know actively
maintained comes with a hosted version
plus local dev support and even
publishes its system prompts so
developers can understand and customize
how the AI thinks. For builders who want
to move fast but still keep a clean
reliable backend instead of a pile of
random code. Chef turns AI assisted
development into a structured back-end
smart app building experience. Project
number seven, Nano GPT. Minimal powerful
GPT training in a few hundred lines.
Let's dive into the powerful GitHub
project NanoGPT created by Andre
Carpathy as one of the simplest and
fastest repositories for training or
fine-tuning medium-sized GPT models
without getting lost in complex
frameworks. NanoGPT is a clean rewrite
of Ming GPT that focuses on real world
performance instead of heavy
abstractions. A compact train.py handles
the full training loop and model. PI
defines the GPT architecture in just a
few hundred lines. Yet, this setup can
reproduce GPT24M
on open web text using modern GPU
hardware. This minimal design makes it
extremely easy for developers to
understand how a GPT actually works
under the hood, customize layers,
context length, data sets, or training
configs, and quickly experiment with
their own language models or domain
specific variants without dealing with
bloated tooling. It integrates smoothly
with popular libraries like PyTorch,
HuggingFace data sets, and tick token.
uses straightforward configs instead of
hidden magic and is MIT licensed so you
can freely use it in research education
or production prototypes. In short,
NanoGPT stands out as a practical
hackable starting point for anyone who
wants to seriously train or fine-tune
GPT style models while fully
understanding and controlling the code
behind them. Project number eight, goose
on machine extensible AI agent that
turns code into real actions. Let's dive
into the another powerful GitHub
project, Goose, an open-source AI agent
from Block that goes far beyond normal
code suggestions by actually installing
dependencies, running commands, editing
files, executing tests, and wiring
together full development workflows
directly on your machine. Goose is
designed as a local first extensible
framework. You can plug in your
preferred LLM, connect model context
protocol and MCP servers, add custom
tools and plugins, and let it operate
inside your real repositories instead of
a sandbox demo, which means it can
tackle real engineering tasks like
refactors, feature implementation,
cleanup, documentation, and diagnostics
endto- end. It treats automation as
recipes that are transparent and
customizable, so teams can standardize
common workflows while still seeing
exactly what the agent is doing at every
step, keeping control instead of handing
over a black box. Goose ships with a
desktop app and CLI, an Apache 2.0 no
license, active development, and an open
community around plugins and roadmap,
making it a serious choice if you want
an on machine AI teammate that can
reliably take LLM output and turn it
into real auditable engineering actions
in your existing stack. Project number
nine, Nano VLLM. Lightweight, high
performance LLM inference in just 1,200
lines. Let's dive into the next powerful
GitHub project. Nano VLLLM, a minimal
yet highly optimized large language
model inference engine built from
scratch to give you near VLM level
performance in an incredibly small and
readable codebase. Nano VLLLM focuses on
one thing, fast, efficient offline
inference for modern LLMs without
drowning you in framework complexity,
delivering a core implementation in
roughly 1,200 lines of Python that
developers can actually understand and
modify. It mirrors the familiar VLM
style API so you can quickly plug in
your own models and workflows. And under
the hood, it packs serious optimizations
like prefix caching, tensor parallelism,
torch compilation, and CUDA support to
squeeze maximum throughput from consumer
GPUs such as an RTX 4070 class device
while staying stable at scale.
Benchmarks in the repo show nano VLM
matching or even slightly beating VLM on
throughput for tested settings, proving
it's not just a toy, but a production
relevant engine for those who want speed
plus transparency. With an MIT license,
active community interest, and a clean
design that's perfect for learning,
debugging, or embedding into your own
serving stack, Nano VLM stands out as a
go-to choice if you want to understand
how high performance LLM inference works
while running real models efficiently on
your own hardware. Project number 10,
Llama.cpp.
Run powerful AI models locally on almost
any device. Let's dive into the last
GitHub project, llama.cpp, CPP, a highly
optimized CC++ engine that makes it
possible to run modern large language
models directly on your own machine,
laptops, desktops, servers, even phones,
and Raspberry Pi without needing huge
cloud GPUs. Built around the GGML
library, Llama CPP is designed for
minimal setup and maximum performance.
It compiles with no heavy dependencies,
supports CPU acceleration with AVX2, AVX
512, Neon AMX, and offers GPU support
via CUDA, Metal, Vulcan, HIP, and more.
So, it can squeeze speed out of whatever
hardware you already have. One of its
biggest strengths is advanced
quantization. It supports multiple low-
formats like 28bit, massively reducing
memory usage and letting you run
surprisingly large models locally while
keeping responses fast and stable. It
doesn't lock you into a single model
either. Llama CDPA works with many
families such as Llama, Llama 2, Llama
3, Mixtral, Mistral, Falcon, DBRX, and
others with a simple command line
interface and built-in HTTP server. So
tools, UIs, and apps can connect using
an open AI style API with its MIT
license, huge community, constant
optimization work, and proven use in
real world local AI setups. llama.cpa
has become the go-to foundation for
anyone who wants private, offline,
customizable LLM inference that runs
efficiently on everyday hardware. What
an incredible week for open-source AI.
The agent ecosystem is officially
exploding. Which project are you cloning
first? Skyvern for web automation, the
deep code agentic coder, or the
optimized nano VLM? Tell me your
favorite project in the comments below.
Don't miss the next round of GitHub
trends. Hit that subscribe button now
and join our dev community to keep your
AI toolkit sharp.